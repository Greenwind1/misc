---
title: "Case Study using insuranceData"
subtitle: "02: Random Forest"
author: "Maxwell"
date: '`r Sys.Date()`'
# bibliography: references.bib
# link-citations: true
# zotero: "My Library"
# abstract: \singlespacing Abstract which has to be long enough to 
#   take multiple lines otherwise one does not see the effect of single-spacing.
output: 
    html_document:
        number_sections: TRUE
        fig_caption: TRUE
        toc: TRUE
        toc_depth: 5
        toc_float: TRUE
        theme:
            bootswatch: yeti  # minty, flatly, litera, lumen, sandstone, spacelab, yeti
            # https://bootswatch.com/
        highlight: espresso  # espresso, tango, zenburn
        code_folding: show
        # code_download: TRUE
        fig_width: 15
editor_options: 
    chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
# knitr::opts_knit$set(root.dir = ".")  # modify with the location of code
```

----

# Preparations {.tabset .tabset-fade .tabset-pills}
## Packages
```{r Packages, echo=TRUE, message=FALSE}
# Data manipulation
library(tidyverse)
# library(data.table)
# library(janitor)  # janitor::row_to_names()
# library(DT)
# library(stringr)
# library(lubridate)

# Visualisation
# library(ggplot2)
# library(ggalt)  # geom_encircle
library(patchwork)
# library(broman)  # plot_crayons()
# library(ggsci)  # Journal palette
# library(rcartocolor)  # display_carto_all(); https://bit.ly/3Itq5kB
# library(extrafont)  # "Candara"
# library(dotwhisker)

# Descriptive Statistics
# library(mice)
# library(ggmice)  # see src/imputation/ggmice_XX.Rmd
# library(psych)
library(skimr)
# library(gtsummary)
library(summarytools)

# ML
library(tidymodels)
library(vip)
library(ale)  # XAI
# library(broom.mixed)
library(tictoc)  # timer
tidymodels::tidymodels_prefer()  # override conflicting methods of other pkgs

# Dataset
library(insuranceData)
```

## Environment
```{r Environment, echo=TRUE, message=FALSE}
# source(file = "utility/environments.R")
# source(file = "utility/helper_functions.R")

cores <- parallel::detectCores()
cores
```


# Dataset {.tabset .tabset-fade .tabset-pills}
```{r}
data("dataCar")
glimpse(dataCar)
# ?dataCar
```


# Simple EDA {.tabset .tabset-fade .tabset-pills}
```{r}
dataCar %>% skimr::skim()
# dataCar %>% dfSummary() %>% stview(file = "output/case-study_01.html")
```


# Preprocessing {.tabset .tabset-fade .tabset-pills}
```{r}
dataCar <- dataCar %>% 
    select(-numclaims, -claimcst0, -X_OBSTAT_) %>% 
    na.omit()
```


# Data splitting and resampling {.tabset .tabset-fade .tabset-pills}
```{r}
set.seed(2023)
init_splits <- initial_split(
    data = dataCar, prop = 0.8, strata = clm
)
```
```{r}
car_train_val <- training(init_splits)
car_test <- testing(init_splits)
folds <- vfold_cv(car_train_val, v = 4, strata = clm)
```


# Create the recipe and features {.tabset .tabset-fade .tabset-pills}
https://recipes.tidymodels.org/reference/index.html  
- all_factor() captures ordered and unordered factors, all_string() captures characters, all_unordered() captures unordered factors and characters, all_ordered() captures ordered factors, all_nominal() captures characters, unordered and ordered factors.
```{r}
rf_recipe <- recipe(clm ~ ., data = car_train_val) %>% 
    # step_log(veh_value) %>% 
    step_mutate_at(clm, fn = ~ as.factor(.)) %>% 
    step_mutate_at(veh_age, agecat, fn = ~ as.ordered(.))
    # step_dummy(all_nominal_predictors()) %>% 
    # step_zv(all_predictors())
summary(rf_recipe)
```

```{r}
car_train_prep <- rf_recipe %>% prep() %>% bake(new_data = NULL)
car_test_prep <- rf_recipe %>% prep() %>% bake(new_data = car_test)
glimpse(car_train_prep)
```


# Build the model {.tabset .tabset-fade .tabset-pills}
```{r}
show_engines("rand_forest")
args(rand_forest)
rf_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
    set_engine("ranger", num.threads = cores - 1, seed = 2023) %>% 
    set_mode("classification")

rf_mod %>% translate()
extract_parameter_set_dials(rf_mod) %>% 
    finalize(car_train_val)  # check parameters for tuning
```


# Create the workflow {.tabset .tabset-fade .tabset-pills}
```{r}
wf <- workflow() %>% 
    add_model(rf_mod) %>% 
    add_recipe(rf_recipe)
```


# Create the grid tibble for hyper parameter tuning {.tabset .tabset-fade .tabset-pills}
```{r}
set.seed(2023)
grid.tibble <- grid_random(extract_parameter_set_dials(rf_mod) %>% 
                               finalize(car_train_val), 
                           size = 5)
grid.tibble
```


# Train and tune the model {.tabset .tabset-fade .tabset-pills}
## Find the best hyper parameters
```{r}
tic()
wf_res <- wf %>% 
    tune_grid(resamples = folds, 
              grid = grid.tibble, 
              control = control_grid(save_pred = TRUE), 
              metrics = metric_set(roc_auc))
toc()
```
```{r}
p.1 <- wf_res %>% 
    collect_metrics() %>% 
    filter(.metric == "roc_auc") %>% 
    ggplot(aes(mtry, mean)) + 
    # geom_line(linewidth = 1, color = "#CF00A3", alpha = 0.7) + 
    geom_point(size = 2, color = "#CF00A3") + 
    # facet_wrap( ~ .metric, scales = "free", nrow = 2) + 
    # scale_color_carto_d() + 
    scale_x_log10(labels = scales::label_number())

p.2 <- wf_res %>% 
    collect_metrics() %>% 
    filter(.metric == "roc_auc") %>% 
    ggplot(aes(min_n, mean)) + 
    # geom_line(linewidth = 1, color = "#CF00A3", alpha = 0.7) + 
    geom_point(size = 2, color = "#CF00A3") + 
    # facet_wrap( ~ .metric, scales = "free", nrow = 2) + 
    # scale_color_carto_d() + 
    scale_x_log10(labels = scales::label_number())

p.1 + p.2
```

## Slice a model with the best hyper parameters
```{r}
top_models <- wf_res %>% 
    show_best("roc_auc", n = 5)
top_models  # tbl_df

# auto selection with metrics
sel_best_model <- wf_res %>% select_best(metric = "roc_auc")
sel_best_model

# manual selection
best_model <- top_models %>% slice(1)
best_model
```

# Finalizing the model {.tabset .tabset-fade .tabset-pills}
- When we set the engine, we add a new argument: `importance = "impurity"`. This will provide variable importance scores for this last model, which gives some insight into which predictors drive model performance.
```{r}
rf_mod_best <- 
    rand_forest(mtry = best_model$mtry, min_n = best_model$min_n, trees = 2000) %>% 
    set_engine("ranger", num.threads = cores - 1, 
               importance = "permutation", seed = 2023) %>% 
    set_mode("classification")
```

```{r}
final_wf <- wf %>% 
    update_model(rf_mod_best)    # or `finalize_workflow`

final_wf
```

- `last_fit()`: Fit the final best model to the training set and evaluate the test set.
```{r}
final_fit <- 
    final_wf %>% 
    last_fit(init_splits)
class(final_fit)  # last_fit, resample_results, tune_results, tbl...
```
```{r}
final_fit %>%
    collect_metrics()

final_fit %>%
    collect_predictions() %>% 
    roc_curve(clm, .pred_0) %>% 
    autoplot()
```

- The `final_fit` object contains a finalized, fitted workflow that you can use for predicting on new data or further understanding the results. You may want to extract this object, using one of the extract_ helper functions.
```{r}
final_fit_wf <- extract_workflow(final_fit)  # returns a workflow object
```

## XAI
### Permutation Importance
- `extract_fit_parsnip()` extract a parsnip class object.
```{r}
final_fit_wf %>% 
    extract_fit_parsnip() %>% 
    vip()
```

### Accumulated Local Effects
- `extract_fit_engine()` extract a backend class object.  
https://cran.r-project.org/web/packages/ale/vignettes/ale-intro.html
```{r}
final_fit_model <- final_fit_wf %>% 
    extract_fit_engine()
```

```{r}
# tic()
# custom_predict <- function(object, newdata) {
#     predict(object, newdata, type = 'response')$predictions[, 2]
# }
# ale_obj <- ale(data = car_test_prep, 
#                y_col = "clm", 
#                model = final_fit_model, 
#                pred_fun = custom_predict)
# toc()
```

```{r}
# names(ale_obj)
```

